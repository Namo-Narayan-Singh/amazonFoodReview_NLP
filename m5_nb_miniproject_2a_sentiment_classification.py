# -*- coding: utf-8 -*-
"""Copy of M5_NB_MiniProject_2A_Sentiment_Classification_Namo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g-E4xSOUHNvtIeyrbr164fcTVliqVxcH

# Advanced Certification Programme in AI and MLOps
## A programme by IISc and TalentSprint
### Mini-Project: Sentiment Classification on Amazon Food Reviews

## Learning Objectives

At the end of the experiment, you will be able to :

* perform data preprocessing, EDA and feature extraction on the Amazon food review dataset
* train an LSTM model for sentiment classification
* modularize the sentiment classification application

## Dataset description

The dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.

Data includes reviews from Oct 1999 - Oct 2012 period, with 568454 reviews, 256059 users, 74258 products, and 260 users with more than 50 reviews each.

The data is in CSV format with below features:

- ***Id:*** Row Id
- ***ProductId:*** Unique identifier for the product
- ***UserId:*** Unqiue identifier for the user
- ***ProfileName:*** Profile name of the user
- ***HelpfulnessNumerator:*** Number of users who found the review helpful
- ***HelpfulnessDenominator:*** Number of users who indicated whether they found the review helpful or not
- ***Score:*** Rating between 1 and 5
- ***Time:*** Timestamp for the review
- ***Summary:*** Brief summary of the review
- ***Text:*** Text of the review

##  Grading = 10 Points

## Information

Companies often receive thousands of reviews regarding their products which can be analysed to get incites on what customers think about their products.

Every positive review highlights the beneficial key features of the product, which can be replicated to other products making them more likable. On the other hand, every negative review highlights the weaknesses of the product, which can be treated as feedback to make improvements.

### Import required packages
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime
from bs4 import BeautifulSoup
import io
import re
import json
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras import layers

#@title Download the dataset
!wget -q https://cdn.iisc.talentsprint.com/AIandMLOps/MiniProjects/Datasets/Reviews.csv
!ls | grep ".csv"

"""**Exercise 1: Read the Reviews.csv dataset**

**Hint:** pd.read_csv()
"""

# YOUR CODE HERE
df = pd.read_csv('/content/Reviews.csv')

# Display the first few rows of the DataFrame
print(df.head())

"""### Pre-processing and EDA

**Exercise 2: Perform below operations on the dataset [1 Mark]**

- Remove unnecessary columns - 'Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator'
- Check missing values
- Add a new `Sentiment` column using `Score` column ('positive' if score >=3)
- Remove duplicates from data considering `Sentiment` and `Text` columns
- Change `Time` in proper format

- **Remove unnecessary columns - 'Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator'**
"""

# YOUR CODE HERE

# Drop a single column ('B') from the DataFrame
df.drop('Id', axis=1, inplace=True)
df.drop('HelpfulnessNumerator', axis=1, inplace=True)
df.drop('HelpfulnessDenominator', axis=1, inplace=True)

print(df.head())

"""- **Check missing values**"""

# YOUR CODE HERE
# Check for null Values
df.isnull().sum()

"""- **Add a new `Sentiment` column using `Score` column**

Consider a review to be negative is the score is less than 3, else positive.
"""

# Add 'Sentiment' column
# YOUR CODE HERE

# Define a function to apply to each row
def condition_based_value(row):
    if row['Score'] > 3:
        return 'Positive'
    else:
        return 'Negative'

# Apply the function to create a new column 'Sentiment'
df['Sentiment'] = df.apply(condition_based_value, axis=1)

print(df.head())

# View the distribution of positive and negative sentiments in the dataset
sns.countplot(x='Sentiment', data = df)
plt.show()

"""- **Remove duplicates from data considering `Sentiment` and `Text` columns**

**Hint:** To check duplicates rows, refer [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html).
"""

# Check duplicates
# YOUR CODE HERE
# Check for duplicate rows
duplicates = df.duplicated()
print(df.shape)
print(duplicates)

# Remove duplicates
# YOUR CODE HERE

# Keep the last occurrence of duplicates
df = df.drop_duplicates()
print(df.shape)

"""- **Change `Time` in proper format**

The Time is in unix time, change it in UTC.

Note: Unix time is a way of representing a timestamp by representing the time as the number of seconds since January 1st, 1970 at 00:00:00 UTC

**Hint:** datetime.fromtimestamp()
"""

# YOUR CODE HERE
# Define a function to apply to each row
def UtcTime_based_value(row):
        return  datetime.utcfromtimestamp(row['Time'])

# Apply the function to create a new column 'Sentiment'
df['Time'] = df.apply(UtcTime_based_value, axis=1)

df.head()

"""**Exercise 3: Identify the `ProductId`s with highest number of positive and negative reviews. Use barplot. [0.5 Marks]**"""

# YOUR CODE HERE

"""**Exercise 4: Identify the `UserId`s who has given highest number of positive and negative reviews**"""

# YOUR CODE HERE

"""### Pre-process `Text` reviews

**Exercise 5: Create functions to perform below tasks: [1.5 Marks]**

- remove HTML, XML, etc. markup and metadata
- remove punctuations
- remove stopwords

- **Remove HTML, XML, etc.**
"""

# YOUR CODE HERE
# removing the html strips
def strip_html(text):
    # BeautifulSoup is a useful library for extracting data from HTML and XML documents
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()


df['Text'] = df['Text'].apply(strip_html)

"""- **Remove punctuations**"""

# YOUR CODE HERE
df.head()
# removing punctuations
def remove_punctuations(text):

    pattern = r'[^a-zA-Z0-9\s]'
    text = re.sub(pattern,'',text)

    # Single character removal
    text = re.sub(r"\s+[a-zA-Z]\s+", ' ', text)

    # Removing multiple spaces
    text = re.sub(r'\s+', ' ', text)

    return text

df['Text'] = df['Text'].apply(remove_punctuations)
df.head()

"""- **Remove stopwords**"""

# YOUR CODE HERE
# setting english stopwords
stopword_list = nltk.corpus.stopwords.words('english')
print(stopword_list)


# Exclude 'not' and its other forms from the stopwords list

updated_stopword_list = []

for word in stopword_list:
    if word=='not' or word.endswith("n't"):
        pass
    else:
        updated_stopword_list.append(word)

print(updated_stopword_list)



# removing the stopwords
def remove_stopwords(text, is_lower_case=False):
    # splitting strings into tokens (list of words)
    tokens = nltk.tokenize.word_tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        # filtering out the stop words
        filtered_tokens = [token for token in tokens if token not in updated_stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in updated_stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text


# Apply function on review column
df['Text'] = df['Text'].apply(remove_stopwords)

df.head()

"""**Exercise 6: Convert `Sentiment` to numerical**"""

# YOUR CODE HERE
# Convert sentiment labels to integers

df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x=="Positive" else 0)

df.head()

"""### Split data into train, validation, and test set

Use 60% for training, 20% for validation, and 20% for testing.
"""

from sklearn.model_selection import train_test_split
# YOUR CODE HERE
X_train, X_test, y_train, y_test = train_test_split(df['Text'].values, df['Sentiment'].values,
                                                    test_size=0.20,
                                                    random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)

"""### Tokenization and padding

**Exercise 7: Convert the review text to sequence of integer values, and make them of uniform length [0.5 Marks]**
"""

# YOUR CODE HERE
# Tokenizer class from the keras.preprocessing.text module creates a word-to-index dictionary
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train_tok = tokenizer.texts_to_sequences(X_train)
X_test_tok = tokenizer.texts_to_sequences(X_test)

# Find the vocabulary size and perform padding on both train and test set
vocab_size = len(tokenizer.word_index) + 1

maxlen = 100

X_train_pad = pad_sequences(X_train_tok, padding='post', maxlen=maxlen, truncating='post')
X_test_pad = pad_sequences(X_test_tok, padding='post', maxlen=maxlen, truncating='post')

print ('number of unique words in the corpus:', vocab_size)

print(X_train[1:2])

"""**Exercise 8: Save the Tokenizer fitted on training set in a json file, and load it back [0.5 Marks]**

**Hint:**

- To get json string from tokenizer, refer [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#to_json).
- To save json string in a json file: `json.dumps()`
- To load json string from a json file: `json.load()`
- To get a tokenizer instance from json string, refer [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/tokenizer_from_json).
"""

def save_tokenizer(tokenizer_to_save):
    # YOUR CODE HERE

def load_tokenizer(filename):
    # YOUR CODE HERE

"""### Build the model using LSTM

**Exercise 9: Create a model using Embedding, LSTM, and Dense layers for sentiment classification [1 Mark]**
"""

# YOUR CODE HERE
EMBEDDING_DIM = 32

print('Build model...')

model = Sequential()
model.add(Embedding(input_dim = vocab_size, output_dim = EMBEDDING_DIM, input_length=maxlen))
model.add(LSTM(units=40,  dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

# Try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print('Summary of the built model...')
print(model.summary())

"""**Exercise 10: Train and evaluate the model**"""

# Train model
# YOUR CODE HERE
history = model.fit(X_train_pad, y_train, batch_size=128, epochs=3, verbose=1, validation_split=0.2)

# Final evaluation of the model on test data
# YOUR CODE HERE
print('Testing...')
y_test = np.array(y_test)
score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)

print('Test score:', score)
print('Test accuracy:', acc)

print("Accuracy: {0:.2%}".format(acc))

"""### Test Prediction

Create function to get sentiment prediction for user input data.
"""

# YOUR CODE HERE
#Let us test some  samples
test_sample = "This product is wonderful!"
test_samples = [test_sample]

test_samples_tokens = tokenizer.texts_to_sequences(test_samples)
test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=maxlen)

# predict
pred = model.predict(x=test_samples_tokens_pad)
pred

"""## Modularization [5 points]

- Modularize the above sentiment classification application for Amazon food reviews dataset as per instructions given in the Instructions document
"""